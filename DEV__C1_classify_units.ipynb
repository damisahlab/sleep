{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import utils__config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z:\\\\Layton\\\\Sleep_083023'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = [\n",
    "    {\n",
    "        'recording_id': 'Feb02',\n",
    "        'recording_length': 2,\n",
    "        'spike_times_path': 'Cache/Subject01/Feb02/S01_spikes.csv',\n",
    "        'spike_forms_path': 'Cache/Subject01/Feb02/S01_spikeforms.csv'\n",
    "    },\n",
    "    {\n",
    "        'recording_id': 'Jul11',\n",
    "        'recording_length': 9.68,\n",
    "        'spike_times_path': 'Cache/Subject05/Jul11/S05_spikes.csv',\n",
    "        'spike_forms_path': 'Cache/Subject05/Jul11/S05_spikeforms.csv'\n",
    "    },\n",
    "    {\n",
    "        'recording_id': 'Jul12',\n",
    "        'recording_length': 10.55,\n",
    "        'spike_times_path': 'Cache/Subject05/Jul12/S05_spikes.csv',\n",
    "        'spike_forms_path': 'Cache/Subject05/Jul12/S05_spikeforms.csv'\n",
    "    },\n",
    "    {\n",
    "        'recording_id': 'Jul13',\n",
    "        'recording_length': 10.40,\n",
    "        'spike_times_path': 'Cache/Subject05/Jul13/S05_spikes.csv',\n",
    "        'spike_forms_path': 'Cache/Subject05/Jul13/S05_spikeforms.csv'\n",
    "    }\n",
    "]\n",
    "\n",
    "output_path = 'Cache/cell_type_metrics.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to store dataframes for waveforms and times\n",
    "waveforms_dfs = []\n",
    "times_dfs = []\n",
    "\n",
    "# Process each recording\n",
    "for recording in recordings:\n",
    "    # Load and process waveforms\n",
    "    waveforms = pd.read_csv(recording['spike_forms_path'])\n",
    "    waveforms = waveforms[['unit_id', 'time_point', 'amplitude']]\n",
    "    waveforms.columns = ['unit_id_old', 'time_point', 'amplitude']\n",
    "    waveforms['unit_id'] = waveforms['unit_id_old'].astype(str) + '_' + recording['recording_id']\n",
    "    waveforms_dfs.append(waveforms)\n",
    "    \n",
    "    # Load and process times\n",
    "    times = pd.read_csv(recording['spike_times_path'])\n",
    "    times = times[['unit_id', 'seconds']]\n",
    "    times.columns = ['unit_id_old', 'time']\n",
    "    times['unit_id'] = times['unit_id_old'].astype(str) + '_' + recording['recording_id']\n",
    "    times['recording_id'] = recording['recording_id']\n",
    "    times_dfs.append(times)\n",
    "\n",
    "# Concatenate all dataframes for waveforms and times\n",
    "waveforms = pd.concat(waveforms_dfs, ignore_index=True)\n",
    "times = pd.concat(times_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unit_id and count the number of spikes for each unit_id\n",
    "spike_counts = times.groupby('unit_id').size()\n",
    "\n",
    "# Extract the recording_id from the unit_id and map it to its corresponding recording length in seconds\n",
    "recording_lengths = {rec['recording_id']: rec['recording_length'] * 3600 for rec in recordings}  # convert hours to seconds\n",
    "times['recording_length_seconds'] = times['recording_id'].map(recording_lengths)\n",
    "\n",
    "# Ensure that each unit_id has the same recording length (this should be the case)\n",
    "recording_lengths_by_unit = times.groupby('unit_id')['recording_length_seconds'].first()\n",
    "\n",
    "# Calculate the average firing rate for each unit_id\n",
    "firing_rates = spike_counts / recording_lengths_by_unit\n",
    "\n",
    "# Convert the Series into a DataFrame\n",
    "firing_rates = firing_rates.reset_index()\n",
    "firing_rates.columns = ['unit_id', 'firing_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trough-to-peak time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def trough_to_peak_times_df(waveforms_df, sampling_rate=30000):\n",
    "    \"\"\"\n",
    "    Calculate the trough-to-peak time for each neuron's average waveform (or will assign NaN if unit is positive-spiking).\n",
    "\n",
    "    Parameters:\n",
    "    - waveforms_df: A pandas DataFrame with columns 'unit_id', 'time_point', and 'amplitude'.\n",
    "    - sampling_rate: The sampling rate in Hz. Default is 30000.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with columns 'unit_id' and 'trough_to_peak_time'.\n",
    "    \"\"\"\n",
    "    # Reshape the data to wide format\n",
    "    waveforms_wide = waveforms_df.pivot(index='time_point', columns='unit_id', values='amplitude')\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    unit_ids = []\n",
    "    times_to_peak = []\n",
    "\n",
    "    # Loop through each unit\n",
    "    for unit in waveforms_wide.columns:\n",
    "        # Check if the unit is positive-spiking\n",
    "        if \"pos\" in unit:\n",
    "            unit_ids.append(unit)\n",
    "            times_to_peak.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # If the unit is negative-spiking, compute the trough-to-peak time\n",
    "        waveform = waveforms_wide[unit].values\n",
    "        trough_idx = np.argmin(waveform)\n",
    "        peak_idx = trough_idx + np.argmax(waveform[trough_idx:])\n",
    "        samples_to_peak = peak_idx - trough_idx\n",
    "        time_to_peak = (samples_to_peak / sampling_rate) * 1000\n",
    "\n",
    "        unit_ids.append(unit)\n",
    "        times_to_peak.append(time_to_peak)\n",
    "\n",
    "    # Create a DataFrame for the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'unit_id': unit_ids,\n",
    "        'trough_to_peak': times_to_peak\n",
    "    })\n",
    "\n",
    "    return results_df\n",
    "\n",
    "ttp_times = trough_to_peak_times_df(waveforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burst Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ISI threshold for bursts (10 ms)\n",
    "threshold = 0.01\n",
    "\n",
    "def calculate_burst_index(group):\n",
    "    # Calculate ISIs\n",
    "    isis = group['time'].diff().dropna()\n",
    "    \n",
    "    # Identify spikes that are part of a burst\n",
    "    burst_spikes = isis[isis < threshold].count() + 1  # + 1 to account for the first spike in each burst\n",
    "    \n",
    "    # Calculate Burst Index\n",
    "    bi = burst_spikes / len(group)\n",
    "    \n",
    "    return bi\n",
    "\n",
    "# Calculate Burst Index for each unit\n",
    "burst_indices = times.groupby('unit_id').apply(calculate_burst_index).reset_index()\n",
    "burst_indices.columns = ['unit_id', 'burst_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge metrics and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = firing_rates.merge(ttp_times, on='unit_id').merge(burst_indices, on='unit_id')\n",
    "merged_df['log_firing_rate'] = np.log2(merged_df['firing_rate'])\n",
    "\n",
    "id_frame = waveforms[['unit_id', 'unit_id_old']].drop_duplicates()\n",
    "final_df = merged_df.merge(id_frame, on='unit_id', how='left')\n",
    "final_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_autocorrelogram(times, bin_size=0.003, window=0.1):\n",
    "#     \"\"\"Plot the autocorrelogram of spike times.\"\"\"\n",
    "#     time_diffs = []\n",
    "#     for i in range(len(times)):\n",
    "#         for j in range(len(times)):\n",
    "#             if i != j:\n",
    "#                 time_diffs.append(times[i] - times[j])\n",
    "    \n",
    "#     bins = np.arange(-window, window + bin_size, bin_size)\n",
    "#     hist, _ = np.histogram(time_diffs, bins=bins)\n",
    "    \n",
    "#     plt.bar(bins[:-1], hist, width=bin_size, align='edge')\n",
    "#     plt.xlabel('Time lag (s)')\n",
    "#     plt.ylabel('Spike count')\n",
    "#     plt.title('Autocorrelogram')\n",
    "#     plt.xlim([-window, window])\n",
    "#     plt.show()\n",
    "\n",
    "# # Generate autocorrelograms for each unit\n",
    "# for unit_id, group in df.groupby('unit_id'):\n",
    "#     print(f\"Unit: {unit_id}\")\n",
    "#     plot_autocorrelogram(group['time'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tau Rise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the triple-exponential function\n",
    "def acg_fit(x, tau_decay, tau_rise, c, d, rateasymptote, trefrac, tau_burst, h):\n",
    "    return np.maximum(c * (np.exp(-(x - trefrac) / tau_decay) - d * np.exp(-(x - trefrac) / tau_rise)) + h * np.exp(-(x - trefrac) / tau_burst) + rateasymptote, 0)\n",
    "\n",
    "def compute_autocorrelogram(spike_times, bin_size=0.0005, max_lag=0.05):\n",
    "    time_diffs = []\n",
    "    \n",
    "    for i in range(len(spike_times)):\n",
    "        diffs = spike_times - spike_times[i]\n",
    "        relevant_diffs = diffs[(diffs > -max_lag) & (diffs < max_lag) & (diffs != 0)]\n",
    "        time_diffs.extend(relevant_diffs)\n",
    "\n",
    "    autocorr, bin_edges = np.histogram(time_diffs, bins=np.arange(-max_lag, max_lag + bin_size, bin_size))\n",
    "    autocorr[int(len(autocorr)/2)] = 0  # Set the value at zero lag to zero\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    return autocorr, bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['unit_id', 'tau_decay', 'tau_rise', 'c', 'd', 'rateasymptote', 'trefrac', 'tau_burst', 'h']\n",
    "a0 = [1, 2, 20, 30, 1.5, 2, 5, 0.5]\n",
    "lb = [0.1, 0, 1, 0, 0.1, 0, 0, -30]\n",
    "ub = [50, 15, 500, 500, 5, 100, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results list\n",
    "results_list = []\n",
    "\n",
    "for unit, group in tqdm(times.groupby('unit_id'), total=times['unit_id'].nunique()):\n",
    "    spike_times = group['time'].values\n",
    "    autocorr, bin_centers = compute_autocorrelogram(spike_times)\n",
    "    \n",
    "    # Fit the data\n",
    "    try:\n",
    "        popt, _ = curve_fit(acg_fit, bin_centers, autocorr, p0=a0, bounds=(lb, ub))\n",
    "        results_list.append([unit] + list(popt))\n",
    "    except:\n",
    "        results_list.append([unit] + [np.nan for _ in range(len(keys) - 1)])\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the first 5 unique unit_ids\n",
    "selected_unit_ids = times['unit_id'].unique()[:5]\n",
    "subset_times = times[times['unit_id'].isin(selected_unit_ids)]\n",
    "\n",
    "for unit, group in subset_times.groupby('unit_id'):\n",
    "    spike_times = group['time'].values\n",
    "    autocorr, bin_centers = compute_autocorrelogram(spike_times)\n",
    "    \n",
    "    try:\n",
    "        popt, _ = curve_fit(acg_fit, bin_centers, autocorr, p0=a0, bounds=(lb, ub))\n",
    "        plt.figure()\n",
    "        plt.plot(bin_centers, autocorr, 'b-', label='Data')\n",
    "        plt.plot(bin_centers, acg_fit(bin_centers, *popt), 'r-', label='Fit')\n",
    "        plt.title(f'Unit {unit}')\n",
    "        plt.legend()\n",
    "    except:\n",
    "        pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 43/122 [01:21<10:20,  7.85s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the triple-exponential function\n",
    "def acg_fit(x, a, b, c, d, e, f, g, h):\n",
    "    return np.maximum(c * (np.exp(-(x - f) / a) - d * np.exp(-(x - f) / b)) + h * np.exp(-(x - f) / g) + e, 0)\n",
    "\n",
    "def compute_autocorrelogram(spike_times, bin_size=0.0005, max_lag=0.05):\n",
    "    time_diffs = []\n",
    "\n",
    "    for i in range(len(spike_times)):\n",
    "        diffs = spike_times - spike_times[i]\n",
    "        relevant_diffs = diffs[(diffs > -max_lag) & (diffs < max_lag) & (diffs != 0)]\n",
    "        time_diffs.extend(relevant_diffs)\n",
    "\n",
    "    autocorr, bin_edges = np.histogram(time_diffs, bins=np.arange(-max_lag, max_lag + bin_size, bin_size))\n",
    "    autocorr[int(len(autocorr)/2)] = 0  # Set the value at zero lag to zero\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    return autocorr, bin_centers\n",
    "\n",
    "# New initial values and bounds from MATLAB implementation\n",
    "a0 = [20, 1, 30, 2, 0.5, 5, 1.5, 2]\n",
    "lb = [1, 0.1, 0, 0, -30, 0, 0.1, 0]\n",
    "ub = [500, 50, 500, 15, 50, 20, 5, 100]\n",
    "\n",
    "results_list = []\n",
    "keys = ['unit_id', 'tau_decay', 'tau_rise', 'c', 'd', 'rateasymptote', 'trefrac', 'tau_burst', 'h']\n",
    "\n",
    "for unit, group in tqdm(times.groupby('unit_id'), total=times['unit_id'].nunique()):\n",
    "    spike_times = group['time'].values\n",
    "    autocorr, bin_centers = compute_autocorrelogram(spike_times)\n",
    "\n",
    "    # Fit the data\n",
    "    try:\n",
    "        popt, _ = curve_fit(acg_fit, bin_centers, autocorr, p0=a0, bounds=(lb, ub))\n",
    "        results_list.append([unit] + list(popt))\n",
    "    except:\n",
    "        results_list.append([unit] + [np.nan for _ in range(len(keys) - 1)])\n",
    "\n",
    "results_df = pd.DataFrame(results_list, columns=keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
