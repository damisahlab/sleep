{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandas.arrays import IntervalArray\n",
    "\n",
    "import mne\n",
    "from mne.time_frequency import tfr_array_morlet\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from utils__helpers_macro import hilbert_powerphase, hilbert_envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils__config\n",
    "os.chdir(utils__config.working_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fif_path = 'Cache/Subject01/Feb02/S01_Feb02_256hz.fif'\n",
    "# sw_path = 'Cache/Subject01/Feb02/S01_SW.csv'\n",
    "# spike_path = 'Cache/Subject01/Feb02/S01_spikes.csv'\n",
    "# legui_path = 'Cache/Subject01/Feb02/S01_electrodes.csv'\n",
    "# bad_channel_path = 'Cache/Subject01/Feb02/S01_bad_channels.csv'\n",
    "# best_channel_path = 'Cache/Subject01/Feb02/S01_best_channels.csv'\n",
    "# output_path = 'Cache/Subject01/Feb02/S01_spike_sw_coupling.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fif_path = 'Cache/Subject05/Jul12/S05_Jul12_256hz.fif'\n",
    "sw_path = 'Cache/Subject05/Jul12/S05_SW.csv'\n",
    "spike_path = 'Cache/Subject05/Jul12/S05_spikes.csv'\n",
    "legui_path = 'Cache/Subject05/S05_electrodes.csv'\n",
    "bad_channel_path = 'Cache/Subject05/Jul12/S05_bad_channels.csv'\n",
    "best_channel_path = 'Cache/Subject05/Jul12/S05_best_channels.csv'\n",
    "output_path = 'Cache/Subject05/Jul12/S05_Jul12_spike_sw_coupling.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 2 # time window (in seconds) to pad before/after the SW\n",
    "\n",
    "# selected_regions = ['superior_frontal_gyrus', 'middle_frontal_gyrus', \n",
    "#                     'inferior_frontal_gyrus', 'medial_frontal_gyrus',\n",
    "#                     'orbitofrontal_gyrus', 'frontal_pole',\n",
    "#                     'subcallosal_gyrus', 'subgenual_cingulate_gyrus', \n",
    "#                     'anterior_cingulate_gyrus']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_fif(fif_path, preload = True, verbose = None)\n",
    "\n",
    "# Select only macroelectrodes\n",
    "raw.pick_types(seeg = True, ecog = True)\n",
    "\n",
    "# Remove rejected channels\n",
    "bad_channels = pd.read_csv(bad_channel_path)\n",
    "bad_channels = bad_channels[bad_channels['channel'].isin(raw.ch_names)]\n",
    "raw.drop_channels(ch_names = bad_channels['channel'].astype('string'))\n",
    "\n",
    "# Select channels with the most SW's in each ROI\n",
    "#best_channels = pd.read_csv(best_channel_path)\n",
    "#raw.pick_channels(ch_names = best_channels['Channel'].tolist())\n",
    "\n",
    "# Select channels in specific ROI's\n",
    "#legui = pd.read_csv(legui_path)\n",
    "#legui = legui.loc[legui['roi_1'].isin(selected_regions)]\n",
    "#raw.pick_channels(ch_names = legui['elec_label'].tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Slow Wave Phase and Envelope (0.3 - 1.5 Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Power and Phase\n",
    "delta = raw.copy()\n",
    "delta = hilbert_powerphase(data = delta, lower = 0.3, upper = 1.5, njobs = 6)\n",
    "delta = delta[['time', 'channel', 'power', 'phase']]\n",
    "\n",
    "# Extract Envelope\n",
    "sw_env = raw.copy()\n",
    "sw_env = hilbert_envelope(sw_env, lower = 0.3, upper = 1.5, njobs = 1)\n",
    "sw_env = sw_env[['time', 'channel', 'envelope']]\n",
    "\n",
    "# Merge Power/Phase and Envelope\n",
    "delta = delta.merge(sw_env, on = ['time', 'channel'])\n",
    "\n",
    "# Calculate z-score of power and envelope\n",
    "delta['log_power'] = 10 * np.log10(delta['power'])\n",
    "delta['zlog_power'] = delta.groupby(['channel'])['log_power'].transform(zscore)\n",
    "\n",
    "delta['z_envelope'] = delta.groupby(['channel'])['envelope'].transform(zscore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersect Spike Times, Slow Wave Intervals, and Envelope/Phase Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Slow Wave data\n",
    "sw_times = pd.read_csv(sw_path)\n",
    "\n",
    "# Merge with LeGUI to get channel laterality\n",
    "legui = pd.read_csv(legui_path)\n",
    "legui = legui[['elec_label', 'hemisphere', 'roi_1']]\n",
    "legui.columns = ['Channel', 'laterality', 'region']\n",
    "sw_times = sw_times.merge(legui, on = 'Channel', how = 'inner')\n",
    "\n",
    "# Select and rename SW columns\n",
    "sw_times = sw_times[['ID', 'Channel', 'laterality', 'region', 'Start', 'End',\n",
    "                     'NegPeak', 'MidCrossing', 'PosPeak', 'ValNegPeak', 'PTP']]\n",
    "sw_times.columns = ['sw_id', 'channel_id', 'sw_laterality', 'sw_region', 'start', 'end',\n",
    "                    'negative_peak', 'mid_crossing', 'positive_peak', 'npeak_amp', 'ptp_amp']\n",
    "\n",
    "# Only keep SW's from channels contained in the final Raw selection\n",
    "sw_times = sw_times[sw_times['channel_id'].isin(raw.ch_names)]\n",
    "\n",
    "# Calculate the z-score of the negative peak value by channel\n",
    "sw_times['zamp_npeak'] = sw_times.groupby('channel_id')['npeak_amp'].transform(zscore)\n",
    "sw_times['zamp_ptp'] = sw_times.groupby('channel_id')['ptp_amp'].transform(zscore)\n",
    "\n",
    "# Expand the SW stop/start times by the window length\n",
    "# in order to include spikes not occurring during the SW\n",
    "sw_times['start'] = sw_times['start'] - time_window\n",
    "sw_times['end'] = sw_times['end'] + time_window\n",
    "\n",
    "# Load Spike data\n",
    "spikes = pd.read_csv(spike_path)\n",
    "spikes = spikes[['unit_id', 'seconds', 'unit_laterality', 'unit_region']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN PHASE DIAGNOSTIC SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Subset sw_times dataframe\n",
    "# sw_times_subset = sw_times[['channel_id', 'negative_peak', 'positive_peak']]\n",
    "\n",
    "# # Step 2: Transform into long format\n",
    "# sw_times_long = pd.melt(sw_times_subset, id_vars=['channel_id'], \n",
    "#                         value_vars=['negative_peak', 'positive_peak'], \n",
    "#                         var_name='peak_type', value_name='time')\n",
    "\n",
    "# # Replace 'negative_peak' and 'positive_peak' with 'negative' and 'positive' for clarity\n",
    "# sw_times_long['peak_type'] = sw_times_long['peak_type'].str.replace('_peak', '')\n",
    "\n",
    "# # Step 3: Subset delta dataframe and rename the 'channel' column to 'channel_id'\n",
    "# delta_subset = delta[['time', 'channel', 'phase']].rename(columns={'channel': 'channel_id'})\n",
    "\n",
    "# # Step 4: Merge asof with merge_asof()\n",
    "# # Ensure the 'time' columns in both dataframes are sorted for merge_asof to work correctly\n",
    "# sw_times_long = sw_times_long.sort_values('time')\n",
    "# delta_subset = delta_subset.sort_values('time')\n",
    "\n",
    "# # Convert 'channel_id' from categorical to string (object)\n",
    "# delta_subset['channel_id'] = delta_subset['channel_id'].astype('str')\n",
    "\n",
    "# # Perform the merge_asof with the renamed column\n",
    "# merged_df = pd.merge_asof(sw_times_long, delta_subset, on='time', by='channel_id', direction='nearest')\n",
    "\n",
    "# # Step 5: Subset the merged dataframe to only include 'peak_type' and 'phase'\n",
    "# final_df = merged_df[['peak_type', 'phase']]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming final_df is your pandas DataFrame with 'peak_type' and 'phase' columns.\n",
    "# # Filtering the DataFrame for positive and negative peak_types\n",
    "# positive_phases = final_df[final_df['peak_type'] == 'positive']['phase']\n",
    "# negative_phases = final_df[final_df['peak_type'] == 'negative']['phase']\n",
    "\n",
    "# # Set the limits for the x-axis\n",
    "# x_limits = (-np.pi, np.pi)\n",
    "\n",
    "# # Create the histograms\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Histogram for positive peak_types\n",
    "# plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "# plt.hist(positive_phases, bins=30, range=x_limits)  # Adjust the number of bins as needed\n",
    "# plt.title('Positive Peaks')\n",
    "# plt.xlabel('Phase (radians)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xlim(x_limits)\n",
    "\n",
    "# # Histogram for negative peak_types\n",
    "# plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "# plt.hist(negative_phases, bins=30, range=x_limits)  # Adjust the number of bins as needed\n",
    "# plt.title('Negative Peaks')\n",
    "# plt.xlabel('Phase (radians)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xlim(x_limits)\n",
    "\n",
    "# # Display the histograms\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END PHASE DIAGNOSTIC SECTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select claustrum spikes that are contained within a slow wave interval; attach slow-wave meta-data to their associated spikes; copy the spikes so that spikes which occur in multiple channels' slow waves are counted for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "for chan in tqdm(sw_times.channel_id.unique()):\n",
    "\n",
    "    # Subset the phase-envelope dataset\n",
    "    sw_data = delta[delta.channel == chan].copy(deep = True)\n",
    "\n",
    "    # Subset the slow-wave dataset\n",
    "    sw_data_2 = sw_times[sw_times.channel_id == chan].copy(deep = True)\n",
    "\n",
    "    # Create an array of intervals denoting the\n",
    "    # timestamps when slow waves were present:\n",
    "    sw_windows = IntervalArray.from_arrays(left = sw_times[sw_times.channel_id == chan]['start'],\n",
    "                                           right = sw_times[sw_times.channel_id == chan]['end'],\n",
    "                                           closed = 'both')\n",
    "    \n",
    "    # Initialize the channel-wise dataset\n",
    "    sw_spikes = pd.DataFrame()\n",
    "\n",
    "    # Select spikes that occur within a slow wave interval\n",
    "    for idx, sw_window in enumerate(sw_windows):\n",
    "\n",
    "        # Generate booleans for spike times within a window\n",
    "        selected_spikes = spikes.seconds.between(left = sw_window.left, \n",
    "                                                 right = sw_window.right, \n",
    "                                                 inclusive = 'both')\n",
    "\n",
    "        # Use booleans to select the actual spike times\n",
    "        sw_spikes_temp = spikes[selected_spikes].copy(deep = True)\n",
    "\n",
    "        # Add SW channel meta-data\n",
    "        sw_spikes_temp['channel_id'] = chan\n",
    "        sw_spikes_temp['channel_region'] = sw_times[sw_times.channel_id == chan]['sw_region'].iloc[0]\n",
    "        sw_spikes_temp['channel_side'] = sw_times[sw_times.channel_id == chan]['sw_laterality'].iloc[0]\n",
    "        \n",
    "        # Take advantage of the fact that the sw_windows index\n",
    "        # will be identical to the sw_data_2 index to get SW meta-data:\n",
    "        sw_spikes_temp['negative_peak'] = sw_spikes_temp['seconds'] - sw_data_2.iloc[idx]['negative_peak']\n",
    "        sw_spikes_temp['mid_crossing'] = sw_spikes_temp['seconds'] - sw_data_2.iloc[idx]['mid_crossing']\n",
    "        sw_spikes_temp['positive_peak'] = sw_spikes_temp['seconds'] - sw_data_2.iloc[idx]['positive_peak']\n",
    "\n",
    "        sw_spikes_temp['start'] = sw_spikes_temp['seconds'] - (sw_data_2.iloc[idx]['start'] + time_window)\n",
    "        sw_spikes_temp['end'] = sw_spikes_temp['seconds'] - (sw_data_2.iloc[idx]['end'] - time_window)\n",
    "\n",
    "        # Add SW meta-data\n",
    "        sw_spikes_temp['sw_id'] = sw_data_2.iloc[idx]['sw_id']\n",
    "        sw_spikes_temp['zamp_npeak'] = sw_data_2.iloc[idx]['zamp_npeak']\n",
    "        sw_spikes_temp['zamp_ptp'] = sw_data_2.iloc[idx]['zamp_ptp']\n",
    "\n",
    "        # Concatenate into channel dataset\n",
    "        sw_spikes = pd.concat((sw_spikes, sw_spikes_temp))\n",
    "\n",
    "    # For every spike, find the nearest \n",
    "    # sample in the phase-envelope dataset...\n",
    "    data_temp = pd.merge_asof(sw_spikes.sort_values('seconds'), sw_data.sort_values('time'), \n",
    "                              left_on = 'seconds', right_on = 'time', direction = 'nearest')\n",
    "\n",
    "    data_temp.drop(columns = ['time', 'channel', 'power'], inplace = True)\n",
    "\n",
    "    # Concatenate into final dataset\n",
    "    data = pd.concat((data, data_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(output_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bceaa3bdda3825794b37c15d2000316b6f4a45a3d4f5e14660beed4f1d5f7638"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
